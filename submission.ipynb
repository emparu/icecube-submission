{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419cb43a",
   "metadata": {
    "papermill": {
     "duration": 0.005142,
     "end_time": "2023-04-17T20:39:42.288111",
     "exception": false,
     "start_time": "2023-04-17T20:39:42.282969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GraphNeT Baseline Submission\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/graphnet-team/graphnet/main/assets/identity/graphnet-logo-and-wordmark.png\" width=\"600\" height=\"600\" />\n",
    "\n",
    "This notebook submits predictions from the public pre-trained dynedge to the leaderboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e742c6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:39:42.298664Z",
     "iopub.status.busy": "2023-04-17T20:39:42.297606Z",
     "iopub.status.idle": "2023-04-17T20:44:26.854700Z",
     "shell.execute_reply": "2023-04-17T20:44:26.853474Z"
    },
    "papermill": {
     "duration": 284.565369,
     "end_time": "2023-04-17T20:44:26.857638",
     "exception": false,
     "start_time": "2023-04-17T20:39:42.292269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'software': No such file or directory\r\n",
      "Processing ./software/dependencies/torch-1.11.0+cu115-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0+cu115) (4.4.0)\r\n",
      "Installing collected packages: torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.13.0\r\n",
      "    Uninstalling torch-1.13.0:\r\n",
      "      Successfully uninstalled torch-1.13.0\r\n",
      "Successfully installed torch-1.11.0+cu115\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing ./software/dependencies/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl\r\n",
      "Installing collected packages: torch-cluster\r\n",
      "Successfully installed torch-cluster-1.6.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing ./software/dependencies/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl\r\n",
      "Installing collected packages: torch-scatter\r\n",
      "Successfully installed torch-scatter-2.0.9\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing ./software/dependencies/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse==0.6.13) (1.7.3)\r\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse==0.6.13) (1.21.6)\r\n",
      "Installing collected packages: torch-sparse\r\n",
      "Successfully installed torch-sparse-0.6.13\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing ./software/dependencies/torch_geometric-2.0.4.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (4.64.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.21.6)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.7.3)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.3.5)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (3.1.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (2.28.2)\r\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (3.0.9)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.0.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric==2.0.4) (2.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric==2.0.4) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric==2.0.4) (2023.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (2.1.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric==2.0.4) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric==2.0.4) (1.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->torch-geometric==2.0.4) (1.16.0)\r\n",
      "Building wheels for collected packages: torch-geometric\r\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616602 sha256=41cd11d76bffa83912c5196a3c56789642af10792480e74402bf92af8b347751\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/33/a3/07aa146f758cd91ebee36268011873ae31c2cfc59dec089e04\r\n",
      "Successfully built torch-geometric\r\n",
      "Installing collected packages: torch-geometric\r\n",
      "Successfully installed torch-geometric-2.0.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mLooking in links: /kaggle/working/software/dependencies\r\n",
      "Obtaining file:///kaggle/working/software/graphnet\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hProcessing /kaggle/working/software/dependencies/awkward-1.8.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: colorlog>=6.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (6.7.0)\r\n",
      "Processing /kaggle/working/software/dependencies/ConfigUpdater-3.1.1-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: dill>=0.3 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.3.6)\r\n",
      "Requirement already satisfied: matplotlib>=3.5 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (3.5.3)\r\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.21.6)\r\n",
      "Requirement already satisfied: pandas>=1.3 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.3.5)\r\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (5.0.0)\r\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.10.4)\r\n",
      "Processing /kaggle/working/software/dependencies/ruamel.yaml-0.17.21-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit_learn>=1.0 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.0.2)\r\n",
      "Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.7.3)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.4 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.4.46)\r\n",
      "Processing /kaggle/working/software/dependencies/timer-0.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: tqdm>=4.64 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (4.64.1)\r\n",
      "Requirement already satisfied: wandb>=0.12 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.14.0)\r\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.11.0+cu115)\r\n",
      "Requirement already satisfied: torch-cluster>=1.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.6.0)\r\n",
      "Requirement already satisfied: torch-scatter>=2.0 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (2.0.9)\r\n",
      "Requirement already satisfied: torch-sparse>=0.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.6.13)\r\n",
      "Requirement already satisfied: torch-geometric>=2.0 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (2.0.4)\r\n",
      "Requirement already satisfied: pytorch-lightning>=1.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.9.4)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from awkward<2.0,>=1.8->graphnet==0.2.4+77.g57fd0f3) (59.8.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (23.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (4.38.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (2.8.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (0.11.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (3.0.9)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (9.4.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.3->graphnet==0.2.4+77.g57fd0f3) (2023.3)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (6.0)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (0.8.0)\r\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (0.11.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (4.4.0)\r\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (2023.1.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>=1.0->graphnet==0.2.4+77.g57fd0f3) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>=1.0->graphnet==0.2.4+77.g57fd0f3) (1.2.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.4->graphnet==0.2.4+77.g57fd0f3) (4.11.4)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.4->graphnet==0.2.4+77.g57fd0f3) (2.0.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (3.1.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2.28.2)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (5.9.3)\r\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (0.1.2)\r\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.4.4)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (0.4.0)\r\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (3.1.30)\r\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.3.2)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.18.0)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (8.1.3)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (3.20.3)\r\n",
      "Processing /kaggle/working/software/dependencies/ruamel.yaml.clib-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl\r\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.16.0)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (3.8.3)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (4.0.10)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.4->graphnet==0.2.4+77.g57fd0f3) (3.11.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (1.26.14)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2.1.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (22.2.0)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (0.13.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (1.8.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (1.3.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (4.0.2)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (5.0.0)\r\n",
      "Installing collected packages: timer, ruamel.yaml.clib, awkward, ruamel.yaml, configupdater, graphnet\r\n",
      "  Running setup.py develop for graphnet\r\n",
      "Successfully installed awkward-1.8.0 configupdater-3.1.1 graphnet-0.2.4+77.g57fd0f3 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 timer-0.2.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Move software to working disk\n",
    "!rm  -r software\n",
    "!scp -r /kaggle/input/graphnet-and-dependencies/software .\n",
    "\n",
    "# Install dependencies\n",
    "!pip install /kaggle/working/software/dependencies/torch-1.11.0+cu115-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install /kaggle/working/software/dependencies/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install /kaggle/working/software/dependencies/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install /kaggle/working/software/dependencies/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install /kaggle/working/software/dependencies/torch_geometric-2.0.4.tar.gz\n",
    "\n",
    "# Install GraphNeT\n",
    "!cd software/graphnet;pip install --no-index --find-links=\"/kaggle/working/software/dependencies\" -e .[torch]\n",
    "\n",
    "# Append to PATH\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/software/graphnet/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df81d8fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:44:26.874654Z",
     "iopub.status.busy": "2023-04-17T20:44:26.874296Z",
     "iopub.status.idle": "2023-04-17T20:44:38.968670Z",
     "shell.execute_reply": "2023-04-17T20:44:38.967495Z"
    },
    "papermill": {
     "duration": 12.105784,
     "end_time": "2023-04-17T20:44:38.971224",
     "exception": false,
     "start_time": "2023-04-17T20:44:26.865440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-04-17 20:44:34 - get_logger - Writing log to \u001b[1mlogs/graphnet_20230417-204434.log\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-04-17 20:44:35 - warn_once - `icecube` not available. Some functionality may be missing.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN3c104impl8GPUTrace13gpuTraceStateE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "# Import Modules\n",
    "import gc\n",
    "import multiprocessing\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from graphnet.data.sqlite.sqlite_utilities import create_table\n",
    "\n",
    "def load_input(meta_batch: pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Will load the corresponding detector readings associated with the meta data batch.\n",
    "        \"\"\"\n",
    "        batch_id = pd.unique(meta_batch['batch_id'])\n",
    "\n",
    "        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n",
    "        \n",
    "        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n",
    "        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z']]\n",
    "        sensor_positions.index = detector_readings.index\n",
    "\n",
    "        for column in sensor_positions.columns:\n",
    "            if column not in detector_readings.columns:\n",
    "                detector_readings[column] = sensor_positions[column]\n",
    "\n",
    "        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n",
    "        return detector_readings.reset_index()\n",
    "\n",
    "def add_to_table(database_path: str,\n",
    "                      df: pd.DataFrame,\n",
    "                      table_name:  str,\n",
    "                      is_primary_key: bool,\n",
    "                      ) -> None:\n",
    "    \"\"\"Writes meta data to sqlite table. \n",
    "\n",
    "    Args:\n",
    "        database_path (str): the path to the database file.\n",
    "        df (pd.DataFrame): the dataframe that is being written to table.\n",
    "        table_name (str, optional): The name of the meta table. Defaults to 'meta_table'.\n",
    "        is_primary_key(bool): Must be True if each row of df corresponds to a unique event_id. Defaults to False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(database_path)\n",
    "        create_table(   columns=  df.columns,\n",
    "                        database_path = database_path, \n",
    "                        table_name = table_name,\n",
    "                        integer_primary_key= is_primary_key,\n",
    "                        index_column = 'event_id')\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if 'already exists' in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise e\n",
    "    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n",
    "    engine.dispose()\n",
    "    return\n",
    "\n",
    "def convert_to_sqlite(meta_data_path: str,\n",
    "                      database_path: str,\n",
    "                      input_data_folder: str,\n",
    "                      batch_size: int = 200000) -> None:\n",
    "    \"\"\"Converts a selection of the Competition's parquet files to a single sqlite database.\n",
    "\n",
    "    Args:\n",
    "        meta_data_path (str): Path to the meta data file.\n",
    "        batch_size (int): the number of rows extracted from meta data file at a time. Keep low for memory efficiency.\n",
    "        database_path (str): path to database. E.g. '/my_folder/data/my_new_database.db'\n",
    "        input_data_folder (str): folder containing the parquet input files.\n",
    "        accepted_batch_ids (List[int]): The batch_ids you want converted. Defaults to None (all batches will be converted)\n",
    "    \"\"\"\n",
    "    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n",
    "    \n",
    "    if not database_path.endswith('.db'):\n",
    "        database_path = database_path+'.db'\n",
    "        \n",
    "    converted_batches = [] \n",
    "    progress_bar = tqdm(total = None)\n",
    "    for meta_data_batch in meta_data_iter:\n",
    "        unique_batch_ids = pd.unique(meta_data_batch['event_id']).tolist()\n",
    "        meta_data_batch  = meta_data_batch.to_pandas()\n",
    "        add_to_table(database_path = database_path,\n",
    "                    df = meta_data_batch,\n",
    "                    table_name='meta_table',\n",
    "                    is_primary_key= True)\n",
    "        pulses = load_input(meta_batch=meta_data_batch, input_data_folder= input_data_folder)\n",
    "        del meta_data_batch # memory\n",
    "        add_to_table(database_path = database_path,\n",
    "                    df = pulses,\n",
    "                    table_name='pulse_table',\n",
    "                    is_primary_key= False)\n",
    "        del pulses # memory\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    del meta_data_iter # memory\n",
    "    print(f'Conversion Complete!. Database available at\\n {database_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3e236a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:44:38.987642Z",
     "iopub.status.busy": "2023-04-17T20:44:38.987310Z",
     "iopub.status.idle": "2023-04-17T20:44:40.244464Z",
     "shell.execute_reply": "2023-04-17T20:44:40.243113Z"
    },
    "papermill": {
     "duration": 1.267743,
     "end_time": "2023-04-17T20:44:40.246793",
     "exception": false,
     "start_time": "2023-04-17T20:44:38.979050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/kaggle/working/test_database.db': No such file or directory\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/test_database.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/test_database.db\n",
      "Conversion Complete!. Database available at\n",
      " /kaggle/working/test_database.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!rm '/kaggle/working/test_database.db'\n",
    "input_data_folder = '/kaggle/input/icecube-neutrinos-in-deep-ice/test'\n",
    "geometry_table = pd.read_csv('/kaggle/input/icecube-neutrinos-in-deep-ice/sensor_geometry.csv')\n",
    "meta_data_path = '/kaggle/input/icecube-neutrinos-in-deep-ice/test_meta.parquet'\n",
    "\n",
    "database_path = '/kaggle/working/test_database'\n",
    "convert_to_sqlite(meta_data_path,\n",
    "                  database_path=database_path,\n",
    "                  input_data_folder=input_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea6effd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:44:40.265211Z",
     "iopub.status.busy": "2023-04-17T20:44:40.264901Z",
     "iopub.status.idle": "2023-04-17T20:44:41.393443Z",
     "shell.execute_reply": "2023-04-17T20:44:41.392380Z"
    },
    "papermill": {
     "duration": 1.140326,
     "end_time": "2023-04-17T20:44:41.396061",
     "exception": false,
     "start_time": "2023-04-17T20:44:40.255735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.optim.adam import Adam\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import DirectionReconstructionWithKappa, ZenithReconstructionWithKappa, AzimuthReconstructionWithKappa\n",
    "from graphnet.training.callbacks import ProgressBar, PiecewiseLinearLR\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss, VonMisesFisher2DLoss\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.utils import make_dataloader\n",
    "from graphnet.utilities.logging import get_logger\n",
    "from pytorch_lightning import Trainer\n",
    "import pandas as pd\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "def build_model(config: Dict[str,Any], train_dataloader: Any) -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n",
    "    )\n",
    "\n",
    "    if config[\"target\"] == 'direction':\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [config[\"target\"] + \"_x\", \n",
    "                              config[\"target\"] + \"_y\", \n",
    "                              config[\"target\"] + \"_z\", \n",
    "                              config[\"target\"] + \"_kappa\" ]\n",
    "        additional_attributes = ['zenith', 'azimuth', 'event_id']\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=Adam,\n",
    "        optimizer_kwargs={\"lr\": 1e-03, \"eps\": 1e-03},\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs={\n",
    "            \"milestones\": [\n",
    "                0,\n",
    "                len(train_dataloader) / 2,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n",
    "            ],\n",
    "            \"factors\": [1e-02, 1, 1e-02],\n",
    "        },\n",
    "        scheduler_config={\n",
    "            \"interval\": \"step\",\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_pretrained_model(config: Dict[str,Any], state_dict_path: str = '/kaggle/input/dynedge-pretrained/dynedge_pretrained_batch_1_to_50/state_dict.pth') -> StandardModel:\n",
    "    train_dataloader, _ = make_dataloaders(config = config)\n",
    "    model = build_model(config = config, \n",
    "                        train_dataloader = train_dataloader)\n",
    "    #model._inference_trainer = Trainer(config['fit'])\n",
    "    model.load_state_dict(state_dict_path)\n",
    "    model.prediction_columns = [config[\"target\"] + \"_x\", \n",
    "                              config[\"target\"] + \"_y\", \n",
    "                              config[\"target\"] + \"_z\", \n",
    "                              config[\"target\"] + \"_kappa\" ]\n",
    "    model.additional_attributes = ['event_id'] #'zenith', 'azimuth',  not available in test data\n",
    "    return model\n",
    "\n",
    "def make_dataloaders(config: Dict[str, Any]) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "    \n",
    "    train_dataloader = make_dataloader(db = config['path'],\n",
    "                                            selection = pd.read_csv(config['train_selection'])[config['index_column']].ravel().tolist() if config['train_selection'] else None,\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = True,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "    \n",
    "    validate_dataloader = make_dataloader(db = config['path'],\n",
    "                                            selection = pd.read_csv(config['validate_selection'])[config['index_column']].ravel().tolist() if config['validate_selection'] else None,\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                          \n",
    "                                            )\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n",
    "def inference(model, config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Applies model to the database specified in config['inference_database_path'] and saves results to disk.\"\"\"\n",
    "    # Make Dataloader\n",
    "    test_dataloader = make_dataloader(db = config['inference_database_path'],\n",
    "                                            selection = None, # Entire database\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = None, # Cannot make labels in test data\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "    \n",
    "    # Get predictions\n",
    "    results = model.predict_as_dataframe(\n",
    "        gpus = [0],\n",
    "        dataloader = test_dataloader,\n",
    "        prediction_columns=model.prediction_columns,\n",
    "        additional_attributes=['event_id']\n",
    "    )\n",
    "    return results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fbf209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:44:41.412556Z",
     "iopub.status.busy": "2023-04-17T20:44:41.412236Z",
     "iopub.status.idle": "2023-04-17T20:44:41.420012Z",
     "shell.execute_reply": "2023-04-17T20:44:41.418905Z"
    },
    "papermill": {
     "duration": 0.018459,
     "end_time": "2023-04-17T20:44:41.422216",
     "exception": false,
     "start_time": "2023-04-17T20:44:41.403757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataframe(df, angle_post_fix = '_reco', vec_post_fix = '') -> pd.DataFrame:\n",
    "    r = np.sqrt(df['direction_x'+ vec_post_fix]**2 + df['direction_y'+ vec_post_fix]**2 + df['direction_z' + vec_post_fix]**2)\n",
    "    df['zenith' + angle_post_fix] = np.arccos(df['direction_z'+ vec_post_fix]/r)\n",
    "    df['azimuth'+ angle_post_fix] = np.arctan2(df['direction_y'+ vec_post_fix],df['direction_x' + vec_post_fix]) #np.sign(results['true_y'])*np.arccos((results['true_x'])/(np.sqrt(results['true_x']**2 + results['true_y']**2)))\n",
    "    df['azimuth'+ angle_post_fix][df['azimuth'  + angle_post_fix]<0] = df['azimuth'  + angle_post_fix][df['azimuth'  +  angle_post_fix]<0] + 2*np.pi \n",
    "\n",
    "    drop_these_columns = []\n",
    "    for column in results.columns:\n",
    "        if column not in ['event_id', 'zenith', 'azimuth']:\n",
    "            drop_these_columns.append(column)\n",
    "    return df.drop(columns = drop_these_columns).iloc[:,[0,2,1]].set_index('event_id')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f21b618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:44:41.438674Z",
     "iopub.status.busy": "2023-04-17T20:44:41.438366Z",
     "iopub.status.idle": "2023-04-17T20:44:46.479381Z",
     "shell.execute_reply": "2023-04-17T20:44:46.477597Z"
    },
    "papermill": {
     "duration": 5.052536,
     "end_time": "2023-04-17T20:44:46.482660",
     "exception": false,
     "start_time": "2023-04-17T20:44:41.430124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-04-17 20:44:41 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-04-17 20:44:41 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-04-17 20:44:41 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7b05efd4574d30b138f7f60222cbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Constants\n",
    "features = FEATURES.KAGGLE\n",
    "truth = TRUTH.KAGGLE\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "        \"path\": '/kaggle/working/test_database.db',\n",
    "        \"inference_database_path\": '/kaggle/working/test_database.db',\n",
    "        \"pulsemap\": 'pulse_table',\n",
    "        \"truth_table\": 'meta_table',\n",
    "        \"features\": features,\n",
    "        \"truth\": truth,\n",
    "        \"index_column\": 'event_id',\n",
    "        \"run_name_tag\": 'graphnet_baseline_submission',\n",
    "        \"batch_size\": 200,\n",
    "        \"num_workers\": 2,\n",
    "        \"target\": 'direction',\n",
    "        \"early_stopping_patience\": 5,\n",
    "        \"fit\": {\n",
    "                \"max_epochs\": 50,\n",
    "                \"gpus\": [0],\n",
    "                \"distribution_strategy\": None,\n",
    "                },\n",
    "        'train_selection': None,\n",
    "        'validate_selection':  None,\n",
    "        'test_selection': None,\n",
    "        'base_dir': 'training'\n",
    "}\n",
    "model   = load_pretrained_model(config = config)\n",
    "submission_df0 = inference(model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0af29d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:44:46.501413Z",
     "iopub.status.busy": "2023-04-17T20:44:46.500537Z",
     "iopub.status.idle": "2023-04-17T20:44:46.507222Z",
     "shell.execute_reply": "2023-04-17T20:44:46.506272Z"
    },
    "papermill": {
     "duration": 0.018481,
     "end_time": "2023-04-17T20:44:46.509345",
     "exception": false,
     "start_time": "2023-04-17T20:44:46.490864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Directories and constants\n",
    "home_dir = \"/kaggle/input/icecube-neutrinos-in-deep-ice/\"\n",
    "test_format = home_dir + 'test/batch_{batch_id:d}.parquet'\n",
    "model_home = \"/kaggle/input/lstmicecubesdata/\"\n",
    "\n",
    "# Model(s)\n",
    "model_names = [\"4347_MAE_1-02076_bin24_pp96_n6_batch2048_epoch29.h5\",\n",
    "               \"4347_MAE_1-02039_bin24_pp96_n6_batch2048_epoch25.h5\", \n",
    "               \"4346_MAE_1-02020_bin24_pp96_n6_batch2048_epoch27.h5\"]\n",
    "model_weights = np.array([0.30, \n",
    "                          0.30,\n",
    "                          0.40])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa483596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:44:46.525842Z",
     "iopub.status.busy": "2023-04-17T20:44:46.525574Z",
     "iopub.status.idle": "2023-04-17T20:45:15.345151Z",
     "shell.execute_reply": "2023-04-17T20:45:15.344229Z"
    },
    "papermill": {
     "duration": 28.838401,
     "end_time": "2023-04-17T20:45:15.355298",
     "exception": false,
     "start_time": "2023-04-17T20:44:46.516897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Model File: 4347_MAE_1-02076_bin24_pp96_n6_batch2048_epoch29.h5\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 96, 6)]           0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, 96, 6)             0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 96, 384)          230400    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 96, 384)          665856    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 384)              665856    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               98560     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 576)               148032    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,808,704\n",
      "Trainable params: 1,808,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "========== Model File: 4347_MAE_1-02039_bin24_pp96_n6_batch2048_epoch25.h5\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 96, 6)]           0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, 96, 6)             0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 96, 384)          230400    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 96, 384)          665856    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 384)              665856    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               98560     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 576)               148032    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,808,704\n",
      "Trainable params: 1,808,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "========== Model File: 4346_MAE_1-02020_bin24_pp96_n6_batch2048_epoch27.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 96, 6)]           0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, 96, 6)             0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 96, 384)          230400    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 96, 384)          665856    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 384)              665856    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               98560     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 576)               148032    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,808,704\n",
      "Trainable params: 1,808,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "==== Model Parameters\n",
      "Bin Numbers: 24\n",
      "Maximum Pulse Count: 96\n",
      "Features Count: 6\n"
     ]
    }
   ],
   "source": [
    "# Load Models\n",
    "models = []\n",
    "for model_name in model_names:\n",
    "    print(f'\\n========== Model File: {model_name}')\n",
    "    \n",
    "    # Load Model\n",
    "    model_path = model_home + model_name\n",
    "    model = tf.keras.models.load_model(model_path, compile = False)\n",
    "    models.append(model)      \n",
    "    \n",
    "    # Model summary\n",
    "    model.summary()\n",
    "    \n",
    "# Get Model Parameters\n",
    "pulse_count = model.inputs[0].shape[1]\n",
    "feature_count = model.inputs[0].shape[2]\n",
    "output_bins = model.layers[-1].weights[0].shape[-1]\n",
    "bin_num = int(np.sqrt(output_bins))\n",
    "\n",
    "# Model Parameter Summary\n",
    "print(\"\\n==== Model Parameters\")\n",
    "print(f\"Bin Numbers: {bin_num}\")\n",
    "print(f\"Maximum Pulse Count: {pulse_count}\")\n",
    "print(f\"Features Count: {feature_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba3fac8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:45:15.381531Z",
     "iopub.status.busy": "2023-04-17T20:45:15.381206Z",
     "iopub.status.idle": "2023-04-17T20:45:15.422896Z",
     "shell.execute_reply": "2023-04-17T20:45:15.421592Z"
    },
    "papermill": {
     "duration": 0.057509,
     "end_time": "2023-04-17T20:45:15.425162",
     "exception": false,
     "start_time": "2023-04-17T20:45:15.367653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time valid length: 6199.700247193777 ns\n",
      "[0.         0.26179939 0.52359878 0.78539816 1.04719755 1.30899694\n",
      " 1.57079633 1.83259571 2.0943951  2.35619449 2.61799388 2.87979327\n",
      " 3.14159265 3.40339204 3.66519143 3.92699082 4.1887902  4.45058959\n",
      " 4.71238898 4.97418837 5.23598776 5.49778714 5.75958653 6.02138592\n",
      " 6.28318531]\n",
      "[0.         0.41113786 0.58568554 0.72273425 0.84106867 0.94796974\n",
      " 1.04719755 1.1410209  1.23095942 1.31811607 1.40334825 1.48736624\n",
      " 1.57079633 1.65422641 1.73824441 1.82347658 1.91063324 2.00057176\n",
      " 2.0943951  2.19362291 2.30052398 2.41885841 2.55590711 2.73045479\n",
      " 3.14159265]\n"
     ]
    }
   ],
   "source": [
    "# Load sensor_geometry\n",
    "sensor_geometry_df = pd.read_csv(home_dir + \"sensor_geometry.csv\")\n",
    "\n",
    "# Get Sensor Information\n",
    "sensor_x = sensor_geometry_df.x\n",
    "sensor_y = sensor_geometry_df.y\n",
    "sensor_z = sensor_geometry_df.z\n",
    "\n",
    "# Detector constants\n",
    "c_const = 0.299792458  # speed of light [m/ns]\n",
    "\n",
    "# Sensor Min / Max Coordinates\n",
    "x_min = sensor_x.min()\n",
    "x_max = sensor_x.max()\n",
    "y_min = sensor_y.min()\n",
    "y_max = sensor_y.max()\n",
    "z_min = sensor_z.min()\n",
    "z_max = sensor_z.max()\n",
    "\n",
    "detector_length = np.sqrt((x_max - x_min)**2 + (y_max - y_min)**2 + (z_max - z_min)**2)\n",
    "t_valid_length = detector_length / c_const\n",
    "\n",
    "print(f\"time valid length: {t_valid_length} ns\")\n",
    "# Create Azimuth Edges\n",
    "azimuth_edges = np.linspace(0, 2 * np.pi, bin_num + 1)\n",
    "print(azimuth_edges)\n",
    "\n",
    "# Create Zenith Edges\n",
    "zenith_edges = []\n",
    "zenith_edges.append(0)\n",
    "for bin_idx in range(1, bin_num):\n",
    "    zenith_edges.append(np.arccos(np.cos(zenith_edges[-1]) - 2 / (bin_num)))\n",
    "zenith_edges.append(np.pi)\n",
    "zenith_edges = np.array(zenith_edges)\n",
    "print(zenith_edges)\n",
    "angle_bin_zenith0 = np.tile(zenith_edges[:-1], bin_num)\n",
    "angle_bin_zenith1 = np.tile(zenith_edges[1:], bin_num)\n",
    "angle_bin_azimuth0 = np.repeat(azimuth_edges[:-1], bin_num)\n",
    "angle_bin_azimuth1 = np.repeat(azimuth_edges[1:], bin_num)\n",
    "\n",
    "angle_bin_area = (angle_bin_azimuth1 - angle_bin_azimuth0) * (np.cos(angle_bin_zenith0) - np.cos(angle_bin_zenith1))\n",
    "angle_bin_vector_sum_x = (np.sin(angle_bin_azimuth1) - np.sin(angle_bin_azimuth0)) * ((angle_bin_zenith1 - angle_bin_zenith0) / 2 - (np.sin(2 * angle_bin_zenith1) - np.sin(2 * angle_bin_zenith0)) / 4)\n",
    "angle_bin_vector_sum_y = (np.cos(angle_bin_azimuth0) - np.cos(angle_bin_azimuth1)) * ((angle_bin_zenith1 - angle_bin_zenith0) / 2 - (np.sin(2 * angle_bin_zenith1) - np.sin(2 * angle_bin_zenith0)) / 4)\n",
    "angle_bin_vector_sum_z = (angle_bin_azimuth1 - angle_bin_azimuth0) * ((np.cos(2 * angle_bin_zenith0) - np.cos(2 * angle_bin_zenith1)) / 4)\n",
    "\n",
    "angle_bin_vector_mean_x = angle_bin_vector_sum_x / angle_bin_area\n",
    "angle_bin_vector_mean_y = angle_bin_vector_sum_y / angle_bin_area\n",
    "angle_bin_vector_mean_z = angle_bin_vector_sum_z / angle_bin_area\n",
    "\n",
    "angle_bin_vector = np.zeros((1, bin_num * bin_num, 3))\n",
    "angle_bin_vector[:, :, 0] = angle_bin_vector_mean_x\n",
    "angle_bin_vector[:, :, 1] = angle_bin_vector_mean_y\n",
    "angle_bin_vector[:, :, 2] = angle_bin_vector_mean_z\n",
    "\n",
    "angle_bin_vector_unit = angle_bin_vector[0].copy()\n",
    "angle_bin_vector_unit /= np.sqrt((angle_bin_vector_unit**2).sum(axis=1).reshape((-1, 1)))\n",
    "def pred_to_angle(pred, epsilon = 1e-8):\n",
    "    # Convert prediction\n",
    "    pred_vector = (pred.reshape((-1, bin_num**2, 1)) * angle_bin_vector).sum(axis = 1)\n",
    "    \n",
    "    # Normalize\n",
    "    pred_vector_norm = np.sqrt((pred_vector**2).sum(axis = 1))\n",
    "    mask = pred_vector_norm < epsilon\n",
    "    pred_vector_norm[mask] = 1\n",
    "    \n",
    "    # Assign <1, 0, 0> to very small vectors (badly predicted)\n",
    "    pred_vector /= pred_vector_norm.reshape((-1, 1))\n",
    "    pred_vector[mask] = np.array([1., 0., 0.])\n",
    "    \n",
    "    # Convert to angle\n",
    "    azimuth = np.arctan2(pred_vector[:, 1], pred_vector[:, 0])\n",
    "    azimuth[azimuth < 0] += 2 * np.pi\n",
    "    zenith = np.arccos(pred_vector[:, 2])\n",
    "    \n",
    "    # Mask bad norm predictions as 0, 0\n",
    "    azimuth[mask] = 0.\n",
    "    zenith[mask] = 0.\n",
    "    \n",
    "    return azimuth, zenith\n",
    "\n",
    "def weighted_vector_ensemble(angles, weight):\n",
    "    # Convert angle to vector\n",
    "    vec_models = list()\n",
    "    for angle in angles:\n",
    "        az, zen = angle\n",
    "        sa = np.sin(az)\n",
    "        ca = np.cos(az)\n",
    "        sz = np.sin(zen)\n",
    "        cz = np.cos(zen)\n",
    "        vec = np.stack([sz * ca, sz * sa, cz], axis=1)\n",
    "        vec_models.append(vec)\n",
    "    vec_models = np.array(vec_models)\n",
    "\n",
    "    # Weighted-mean\n",
    "    vec_mean = (weight.reshape((-1, 1, 1)) * vec_models).sum(axis=0) / weight.sum()\n",
    "    vec_mean /= np.sqrt((vec_mean**2).sum(axis=1)).reshape((-1, 1))\n",
    "\n",
    "    # Convert vector to angle\n",
    "    zenith = np.arccos(vec_mean[:, 2])\n",
    "    azimuth = np.arctan2(vec_mean[:, 1], vec_mean[:, 0])\n",
    "    azimuth[azimuth < 0] += 2 * np.pi\n",
    "    \n",
    "    return azimuth, zenith\n",
    "# Placeholder\n",
    "open_batch_dict = dict()\n",
    "\n",
    "# Read single event from batch_meta_df\n",
    "def read_event(event_idx, batch_meta_df, pulse_count):\n",
    "    # Read metadata\n",
    "    batch_id, first_pulse_index, last_pulse_index = batch_meta_df.iloc[event_idx][[\"batch_id\", \"first_pulse_index\", \"last_pulse_index\"]].astype(\"int\")\n",
    "\n",
    "    # close past batch df\n",
    "    if batch_id - 1 in open_batch_dict.keys():\n",
    "        del open_batch_dict[batch_id - 1]\n",
    "\n",
    "    # open current batch df\n",
    "    if batch_id not in open_batch_dict.keys():\n",
    "        open_batch_dict.update({batch_id: pd.read_parquet(test_format.format(batch_id=batch_id))})\n",
    "    \n",
    "    batch_df = open_batch_dict[batch_id]\n",
    "    \n",
    "    # Read event\n",
    "    event_feature = batch_df[first_pulse_index:last_pulse_index + 1]\n",
    "    sensor_id = event_feature.sensor_id\n",
    "    \n",
    "    # Merge features into single structured array\n",
    "    dtype = [(\"time\", \"float16\"),\n",
    "             (\"charge\", \"float16\"),\n",
    "             (\"auxiliary\", \"float16\"),\n",
    "             (\"x\", \"float16\"),\n",
    "             (\"y\", \"float16\"),\n",
    "             (\"z\", \"float16\"),\n",
    "             (\"rank\", \"short\")]    \n",
    "    \n",
    "    # Create event_x\n",
    "    event_x = np.zeros(last_pulse_index - first_pulse_index + 1, dtype)\n",
    "    event_x[\"time\"] = event_feature.time.values - event_feature.time.min()\n",
    "    event_x[\"charge\"] = event_feature.charge.values\n",
    "    event_x[\"auxiliary\"] = event_feature.auxiliary.values\n",
    "    event_x[\"x\"] = sensor_geometry_df.x[sensor_id].values\n",
    "    event_x[\"y\"] = sensor_geometry_df.y[sensor_id].values\n",
    "    event_x[\"z\"] = sensor_geometry_df.z[sensor_id].values\n",
    "\n",
    "    # For long event, pick-up\n",
    "    if len(event_x) > pulse_count:\n",
    "        # Find valid time window\n",
    "        t_peak = event_x[\"time\"][event_x[\"charge\"].argmax()]\n",
    "        t_valid_min = t_peak - t_valid_length\n",
    "        t_valid_max = t_peak + t_valid_length\n",
    "        t_valid = (event_x[\"time\"] > t_valid_min) * (event_x[\"time\"] < t_valid_max)\n",
    "\n",
    "        # Rank\n",
    "        event_x[\"rank\"] = 2 * (1 - event_x[\"auxiliary\"]) + (t_valid)\n",
    "\n",
    "        # Sort by Rank and Charge (important goes to backward)\n",
    "        event_x = np.sort(event_x, order = [\"rank\", \"charge\"])\n",
    "\n",
    "        # pick-up from backward\n",
    "        event_x = event_x[-pulse_count:]\n",
    "\n",
    "        # Sort events by time \n",
    "        event_x = np.sort(event_x, order = \"time\")\n",
    "\n",
    "    return event_idx, len(event_x), event_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc7f8623",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:45:15.451234Z",
     "iopub.status.busy": "2023-04-17T20:45:15.450919Z",
     "iopub.status.idle": "2023-04-17T20:45:15.469182Z",
     "shell.execute_reply": "2023-04-17T20:45:15.468264Z"
    },
    "papermill": {
     "duration": 0.033661,
     "end_time": "2023-04-17T20:45:15.471502",
     "exception": false,
     "start_time": "2023-04-17T20:45:15.437841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Test Meta data\n",
    "test_meta_df = pq.read_table(home_dir + 'test_meta.parquet').to_pandas()\n",
    "batch_counts = test_meta_df.batch_id.value_counts().sort_index()\n",
    "\n",
    "batch_max_index = batch_counts.cumsum()\n",
    "batch_max_index[test_meta_df.batch_id.min() - 1] = 0\n",
    "batch_max_index = batch_max_index.sort_index()\n",
    "\n",
    "# Support Function\n",
    "def test_meta_df_spliter(batch_id):\n",
    "    return test_meta_df.loc[batch_max_index[batch_id - 1]:batch_max_index[batch_id] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e32d099c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:45:15.497220Z",
     "iopub.status.busy": "2023-04-17T20:45:15.496904Z",
     "iopub.status.idle": "2023-04-17T20:45:15.639250Z",
     "shell.execute_reply": "2023-04-17T20:45:15.638259Z"
    },
    "papermill": {
     "duration": 0.157713,
     "end_time": "2023-04-17T20:45:15.641313",
     "exception": false,
     "start_time": "2023-04-17T20:45:15.483600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1733)]       0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1733, 1)      0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           34680       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 1731, 1)      0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1731)         36351       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1731)         0           ['cropping1d[0][0]']             \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 1731)         0           ['dense_1[0][0]',                \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3)            5196        ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 76,227\n",
      "Trainable params: 76,227\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def MeanAngErr(y_true, y_pred):\n",
    "    fcos=tf.math.scalar_mul(-0.99999,tf.keras.losses.cosine_similarity(y_true, y_pred))\n",
    "    return tf.reduce_mean(tf.math.acos(fcos), axis=-1)  # Note the `axis=-1`\n",
    "def create_model():\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(1733,))\n",
    "    \n",
    "    intermediate = tf.keras.layers.Reshape((1733,1), input_shape=(1733,))(inputs)\n",
    "\n",
    "    inputs1 = tf.keras.layers.Cropping1D(cropping=(2,0))(intermediate)\n",
    "    inputs1 = tf.keras.layers.Reshape((1731,), input_shape=(1731,1))(inputs1)\n",
    "    \n",
    "    tensor_1 = tf.keras.layers.Dense(20, activation='tanh')(inputs)\n",
    "    tensor_1 = tf.keras.layers.Dense(1731, activation='tanh')(tensor_1)\n",
    "    #tensor_2 = tf.keras.layers.BatchNormalization()(tensor_2)\n",
    "    \n",
    "    #tensor_2 = tf.keras.layers.Dropout(0.5)(tensor_2)\n",
    "    \n",
    "    product = tf.keras.layers.Multiply()([tensor_1, inputs1])\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(3, activation = 'linear')(product)\n",
    "        \n",
    "        # Finalize Model\n",
    "    model = tf.keras.models.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "        # Compile model\n",
    "    model.compile(loss = MeanAngErr,\n",
    "                      optimizer= tf.keras.optimizers.Adam())\n",
    "        \n",
    "        # Show Model Summary\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelens=create_model()\n",
    "modelens.load_weights(\"/kaggle/input/modelens3/model3(1).h5\")\n",
    "\n",
    "def proc_X_models(X,submission_df0,testmeta):\n",
    "    ans=(np.concatenate((models[0].predict(X, verbose=0,batch_size=1024),models[1].predict(X, verbose=0,batch_size=1024),models[2].predict(X, verbose=0,batch_size=1024)),axis=1))\n",
    "    #e1=np.expand_dims(np.sum(np.log(ans[:,0:575]+1e-6)*ans[:,0:575],axis=1),axis=-1)\n",
    "    #e2=np.expand_dims(np.sum(np.log(ans[:,576:1151]+1e-6)*ans[:,576:1151],axis=1),axis=-1)\n",
    "    #e3=np.expand_dims(np.sum(np.log(ans[:,1152:1727]+1e-6)*ans[:,1152:1727],axis=1),axis=-1)\n",
    "    \n",
    "    submission_df0= submission_df0[['direction_x','direction_y','direction_z','direction_kappa']].to_numpy()\n",
    "    \n",
    "    pulses=testmeta[['last_pulse_index']].to_numpy()-testmeta[['first_pulse_index']].to_numpy()\n",
    "    \n",
    "    ans=np.concatenate((ans,submission_df0,pulses),axis=1)\n",
    "    \n",
    "    ans=modelens.predict(ans,batch_size=1024)\n",
    "    #print(ans)\n",
    "    norm=(np.maximum(np.sqrt((ans*ans)@np.array([1,1,1])),0.0000000000001))\n",
    "    \n",
    "    ans[:,0]=(ans[:,0]/norm)*0.9999999\n",
    "    ans[:,1]=(ans[:,1]/norm)*0.9999999\n",
    "    ans[:,2]=(ans[:,2]/norm)*0.9999999\n",
    "    #print(ans)\n",
    "    az=np.arctan2(ans[:,1],ans[:,0])*0.9999999\n",
    "    zen=np.arccos(ans[:,2])\n",
    "    az[az<0]+=2*np.pi-0.0000000000001\n",
    "    return az,zen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f1c66d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:45:15.670113Z",
     "iopub.status.busy": "2023-04-17T20:45:15.669766Z",
     "iopub.status.idle": "2023-04-17T20:45:15.675477Z",
     "shell.execute_reply": "2023-04-17T20:45:15.674385Z"
    },
    "papermill": {
     "duration": 0.02225,
     "end_time": "2023-04-17T20:45:15.677599",
     "exception": false,
     "start_time": "2023-04-17T20:45:15.655349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df0['event_id']=submission_df0['event_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0dedbf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:45:15.705243Z",
     "iopub.status.busy": "2023-04-17T20:45:15.704982Z",
     "iopub.status.idle": "2023-04-17T20:45:40.750500Z",
     "shell.execute_reply": "2023-04-17T20:45:40.749327Z"
    },
    "papermill": {
     "duration": 25.062475,
     "end_time": "2023-04-17T20:45:40.753294",
     "exception": false,
     "start_time": "2023-04-17T20:45:15.690819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get Batch IDs\n",
    "test_batch_ids = test_meta_df.batch_id.unique()\n",
    "\n",
    "# Submission Placeholders\n",
    "test_event_id = []\n",
    "test_azimuth = []\n",
    "test_zenith = []\n",
    "\n",
    "# Batch Loop\n",
    "for batch_id in test_batch_ids:\n",
    "    # Batch Meta DF\n",
    "    batch_meta_df = test_meta_df_spliter(batch_id)\n",
    "\n",
    "    # Set Pulses\n",
    "    test_x = np.zeros((len(batch_meta_df), pulse_count, feature_count), dtype = \"float16\")    \n",
    "    test_x[:, :, 2] = -1    \n",
    "\n",
    "    # Read Event Data\n",
    "    def read_event_local(event_idx):\n",
    "        return read_event(event_idx, batch_meta_df, pulse_count)\n",
    "    \n",
    "    # Multiprocess Events\n",
    "    iterator = range(len(batch_meta_df))\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        for event_idx, pulsecount, event_x in pool.map(read_event_local, iterator):\n",
    "            # Features\n",
    "            test_x[event_idx, :pulsecount, 0] = event_x[\"time\"]\n",
    "            test_x[event_idx, :pulsecount, 1] = event_x[\"charge\"]\n",
    "            test_x[event_idx, :pulsecount, 2] = event_x[\"auxiliary\"]\n",
    "            test_x[event_idx, :pulsecount, 3] = event_x[\"x\"]\n",
    "            test_x[event_idx, :pulsecount, 4] = event_x[\"y\"]\n",
    "            test_x[event_idx, :pulsecount, 5] = event_x[\"z\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Normalize\n",
    "    test_x[:, :, 0] /= 1000  # time\n",
    "    test_x[:, :, 1] /= 300  # charge\n",
    "    test_x[:, :, 3:] /= 600  # space\n",
    "        \n",
    "    # Predict\n",
    "    pred_angles = []\n",
    "   \n",
    "    \n",
    "    # Get Event IDs\n",
    "    event_ids = test_meta_df[test_meta_df.batch_id == batch_id]\n",
    "    \n",
    "    graphpred=pd.merge(event_ids,submission_df0,on='event_id', how='left')\n",
    "\n",
    "    pred_azimuth, pred_zenith = proc_X_models(test_x,graphpred,event_ids)\n",
    "    gc.collect()\n",
    "    \n",
    "    del batch_meta_df\n",
    "    # Get Predicted Azimuth and Zenith\n",
    "    \n",
    "\n",
    "    event_ids=event_ids.event_id.values\n",
    "    \n",
    "    # Finalize \n",
    "    for event_id, azimuth, zenith in zip(event_ids, pred_azimuth, pred_zenith):\n",
    "        if np.isfinite(azimuth) and np.isfinite(zenith):\n",
    "            test_event_id.append(int(event_id))\n",
    "            test_azimuth.append(azimuth)\n",
    "            test_zenith.append(zenith)\n",
    "        else:\n",
    "            test_event_id.append(int(event_id))\n",
    "            test_azimuth.append(0.)\n",
    "            test_zenith.append(0.)\n",
    "# Create and Save Submission.csv\n",
    "submission_df1 = pd.DataFrame({\"event_id\": test_event_id,\n",
    "                              \"azimuth\": test_azimuth,\n",
    "                              \"zenith\": test_zenith})\n",
    "submission_df1 = submission_df1.sort_values(by = ['event_id'])\n",
    "submission_df1.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f855be1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:45:40.782313Z",
     "iopub.status.busy": "2023-04-17T20:45:40.781998Z",
     "iopub.status.idle": "2023-04-17T20:45:40.795899Z",
     "shell.execute_reply": "2023-04-17T20:45:40.794765Z"
    },
    "papermill": {
     "duration": 0.030755,
     "end_time": "2023-04-17T20:45:40.798482",
     "exception": false,
     "start_time": "2023-04-17T20:45:40.767727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>azimuth</th>\n",
       "      <th>zenith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2092</td>\n",
       "      <td>0.481458</td>\n",
       "      <td>1.527498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7344</td>\n",
       "      <td>3.497879</td>\n",
       "      <td>2.469359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9482</td>\n",
       "      <td>4.570243</td>\n",
       "      <td>1.538434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id   azimuth    zenith\n",
       "0      2092  0.481458  1.527498\n",
       "1      7344  3.497879  2.469359\n",
       "2      9482  4.570243  1.538434"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 370.983556,
   "end_time": "2023-04-17T20:45:44.083663",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-17T20:39:33.100107",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "14d85af778af4e7abc2d10754699fc3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3ce4fb579dff4085956a7af1e6b3a090": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "55a64e56b41f47c99893557e19e49dbe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "562fe3a3232441b2a1f5d718798b6809": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_acabb6928c3343878ae49c535089cdf6",
       "placeholder": "​",
       "style": "IPY_MODEL_bd082f28e15a4a0a8268b976ecda5007",
       "value": "Predicting DataLoader 0: 100%"
      }
     },
     "7bcb79c1ebac46479a20c596137df3c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3ce4fb579dff4085956a7af1e6b3a090",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_14d85af778af4e7abc2d10754699fc3a",
       "value": 1.0
      }
     },
     "9bb7b18fa06b4b8d89ee96a2a19daa9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ab7b05efd4574d30b138f7f60222cbd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_562fe3a3232441b2a1f5d718798b6809",
        "IPY_MODEL_7bcb79c1ebac46479a20c596137df3c1",
        "IPY_MODEL_b9c5a8dbbc734ceba2d62dbd8d10421b"
       ],
       "layout": "IPY_MODEL_55a64e56b41f47c99893557e19e49dbe"
      }
     },
     "acabb6928c3343878ae49c535089cdf6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9c5a8dbbc734ceba2d62dbd8d10421b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d25b2a338da2443eaab28ff25bc107cf",
       "placeholder": "​",
       "style": "IPY_MODEL_9bb7b18fa06b4b8d89ee96a2a19daa9d",
       "value": " 1/1 [00:02&lt;00:00,  2.03s/it]"
      }
     },
     "bd082f28e15a4a0a8268b976ecda5007": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d25b2a338da2443eaab28ff25bc107cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
